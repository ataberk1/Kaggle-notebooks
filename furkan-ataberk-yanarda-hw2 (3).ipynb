{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":88839,"databundleVersionId":10191246,"sourceType":"competition"}],"dockerImageVersionId":30786,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Setting Up Work Environment","metadata":{}},{"cell_type":"code","source":"\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2024-11-27T20:26:27.202209Z","iopub.execute_input":"2024-11-27T20:26:27.202612Z","iopub.status.idle":"2024-11-27T20:26:27.222075Z","shell.execute_reply.started":"2024-11-27T20:26:27.202577Z","shell.execute_reply":"2024-11-27T20:26:27.220601Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"data_train = pd.read_csv(\"/kaggle/input/math482-2024-2025-1-hw-02-v2/train.csv\") #reading train data\ndata_test = pd.read_csv(\"/kaggle/input/math482-2024-2025-1-hw-02-v2/test.csv\") #reading test data","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-27T20:26:27.224202Z","iopub.execute_input":"2024-11-27T20:26:27.224749Z","iopub.status.idle":"2024-11-27T20:26:27.749359Z","shell.execute_reply.started":"2024-11-27T20:26:27.224700Z","shell.execute_reply":"2024-11-27T20:26:27.748381Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Overview  of the Data","metadata":{}},{"cell_type":"markdown","source":"**Training Data**\n\nThe training dataset contains 35,000 rows and 28 features.\n\n**Features**:  Include abstract column names that suggest no information about the data. However, all of the columns are numerical.\n\n**Target Variable**:  Abstract \"0\", \"1\", \"2\" and \"3\" values.\n\n**Missing Values**:  feature_08, feature_09, feature_21 and feature_25 has missing values.","metadata":{}},{"cell_type":"code","source":"data_train.info()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-27T20:26:27.751033Z","iopub.execute_input":"2024-11-27T20:26:27.751429Z","iopub.status.idle":"2024-11-27T20:26:27.768286Z","shell.execute_reply.started":"2024-11-27T20:26:27.751393Z","shell.execute_reply":"2024-11-27T20:26:27.766989Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Check for missing values in the dataset\nmissing_values = data_train.isnull().sum()\n\n# Display only columns with missing values\nmissing_columns = missing_values[missing_values > 0]\n\nif missing_columns.empty:\n    print(\"No missing values in the dataset.\")\nelse:\n    print(\"Missing values found:\")\n    print(missing_columns)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-27T20:26:27.770197Z","iopub.execute_input":"2024-11-27T20:26:27.770530Z","iopub.status.idle":"2024-11-27T20:26:27.780229Z","shell.execute_reply.started":"2024-11-27T20:26:27.770497Z","shell.execute_reply":"2024-11-27T20:26:27.779144Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"**Test Data**\r\n\r\nThe test dataset contains510,000 rows an271featuresns and there are no missing values. It matches the structure of the training data.","metadata":{}},{"cell_type":"code","source":"data_test.info()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-27T20:26:27.781570Z","iopub.execute_input":"2024-11-27T20:26:27.781937Z","iopub.status.idle":"2024-11-27T20:26:27.801299Z","shell.execute_reply.started":"2024-11-27T20:26:27.781904Z","shell.execute_reply":"2024-11-27T20:26:27.800132Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Check for missing values in the dataset\nmissing_values = data_test.isnull().sum()\n\n# Display only columns with missing values\nmissing_columns = missing_values[missing_values > 0]\n\nif missing_columns.empty:\n    print(\"No missing values in the dataset.\")\nelse:\n    print(\"Missing values found:\")\n    print(missing_columns)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-27T20:26:27.803868Z","iopub.execute_input":"2024-11-27T20:26:27.804243Z","iopub.status.idle":"2024-11-27T20:26:27.812217Z","shell.execute_reply.started":"2024-11-27T20:26:27.804209Z","shell.execute_reply":"2024-11-27T20:26:27.811309Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Data Preprocessing","metadata":{}},{"cell_type":"markdown","source":"I have to handle missing values first to be able to process the data without error later. Since the missing values are so few compared to the size of the total data, I will simply remove them from the data. Because I don't want to introduce unintentional synthetic relations within the data by using imputation.","metadata":{}},{"cell_type":"code","source":"data_train.dropna(inplace=True)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-27T20:26:27.813619Z","iopub.execute_input":"2024-11-27T20:26:27.813949Z","iopub.status.idle":"2024-11-27T20:26:27.836275Z","shell.execute_reply.started":"2024-11-27T20:26:27.813916Z","shell.execute_reply":"2024-11-27T20:26:27.835157Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"The correlation matrix suggests there are no highly correlated features. However, the correlation matrix displays the correlation coefficient between pairs of features. If a feature is a linear combination of 2 or more features, this method can't capture that relation. So we have to use a better method to find redundant features.","metadata":{}},{"cell_type":"code","source":"# Correlation matrix\ncorrelation_matrix = data_train.loc[:, data_train.columns != \"id\"].corr()\n\n# Heatmap of correlations\nplt.figure(figsize=(12, 8))\nsns.heatmap(correlation_matrix, annot=False, cmap=\"coolwarm\")\nplt.title(\"Correlation Matrix\")\nplt.show()\n\n\n# Flatten the correlation matrix, keeping only the upper triangle (to avoid duplicates)\ncorrelation_pairs = correlation_matrix.where(np.triu(np.ones(correlation_matrix.shape), k=1).astype(bool))\n\n# Unstack the upper triangle into a DataFrame\ncorrelated_features = correlation_pairs.unstack().dropna().abs().sort_values(ascending=False)\n\n# Filter pairs with high correlation\nhigh_correlation_pairs = correlated_features[correlated_features > 0.3]\n\n# Display the most correlated feature pairs\nprint(\"Most Correlated Feature Pairs:\")\nprint(high_correlation_pairs)\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-27T20:26:27.837681Z","iopub.execute_input":"2024-11-27T20:26:27.838017Z","iopub.status.idle":"2024-11-27T20:26:28.604309Z","shell.execute_reply.started":"2024-11-27T20:26:27.837983Z","shell.execute_reply":"2024-11-27T20:26:28.603260Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"I will use the variance inflation factor(VIF) method to find features that are linearly dependent on other features. VIF is calculated as:\nRegress the feature $X_j$ as $X_j = β_1​X_1​+β_2​X_2​+⋯+β_{j−1​}X_{j−1}​+β_{j+1}​X_{j+1}​+⋯+β_p​X_p​+ϵ$, then the VIF of $Xj\\$ is:\n\nVIF= $\\frac{1}{1-R^2}$. \n\nWhere $R^2$ is the coefficient of determination calculated as:\n​\n$ R^2 = SS_e/SS_t$\n\n$SS_e$ = explained varience of $X_j$ calculated as $SS_e = \\sum\\limits_{i=1}^{n} {\\hat{X_{j,i}}​−\\bar{X_j}​)}$ ($\\hat{X_{j,i}}$ is the predicted value of $X_j$ in observation i using the regression above)\n\n$SS_t$ = total varience of $X_j$ calculated as $SS_t​ = \\sum\\limits_{i=1}^{n} {X_{j,i}​−\\bar{X_j}​)}$ ($X_{j,i}$ is the actual value of $X_j$ in ith observation)\n\nIf $R^2$ is 1, this means the $X_j$ can be predicted completely by using other features (linearly dependent) and offers no new information. If it is 0, then $X_j$ is unique and can't be predicted by other features. Then, high VIF means the feature is likely to be linear combination of other features and redundant.\n","metadata":{}},{"cell_type":"code","source":"from statsmodels.stats.outliers_influence import variance_inflation_factor\nfrom sklearn.preprocessing import StandardScaler\n\nX_train = data_train.drop(columns=['target','id'])\n\n# Standardize the features so that features with large scale don't dominate the result\nscaler = StandardScaler()\nscaled_features = scaler.fit_transform(X_train)\n\n# Create a DataFrame for scaled features\nscaled_data = pd.DataFrame(scaled_features, columns=X_train.columns)\n\n# Calculate VIF for each feature\ndef calculate_vif(dataframe):\n    vif_data = pd.DataFrame()\n    vif_data[\"Feature\"] = dataframe.columns\n    vif_data[\"VIF\"] = [variance_inflation_factor(dataframe.values, i) for i in range(dataframe.shape[1])]\n    return vif_data\n\nvif_results = calculate_vif(scaled_data)\n\n# Filter features with VIF > 10\nhigh_vif_features = vif_results[vif_results[\"VIF\"] > 10][\"Feature\"]\n\n# Display features with high VIF values\nprint(\"Features with VIF > 10:\")\nprint(high_vif_features)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-27T20:26:28.605758Z","iopub.execute_input":"2024-11-27T20:26:28.606230Z","iopub.status.idle":"2024-11-27T20:26:31.093127Z","shell.execute_reply.started":"2024-11-27T20:26:28.606182Z","shell.execute_reply":"2024-11-27T20:26:31.091260Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"We have the features that have high VIF values and now we will remove them all because they bring no new information.","metadata":{}},{"cell_type":"code","source":"final_data = scaled_data.drop(columns = [\"feature_05\",\"feature_20\",\"feature_08\",\"feature_01\"])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-27T20:26:31.094515Z","iopub.execute_input":"2024-11-27T20:26:31.094926Z","iopub.status.idle":"2024-11-27T20:26:31.104138Z","shell.execute_reply.started":"2024-11-27T20:26:31.094880Z","shell.execute_reply":"2024-11-27T20:26:31.101880Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"I will check for the outliers in the remaining columns using boxplot. If there are features with many outliers, I will remove them using the IQR method. I don't see any problem in removing the outliers because none of the features are highly correlated with the target. ","metadata":{}},{"cell_type":"code","source":"# List of features to visualize\nfeatures_to_plot = final_data.columns\n\n# Create boxplots for each feature\nplt.figure(figsize=(12, 24))\nfor i, feature in enumerate(features_to_plot, 1):\n    plt.subplot(8, 3, i)  # Arrange plots in a grid (2 rows, 3 columns)\n    sns.boxplot(data=data_train[feature])\n    plt.title(f\"Boxplot for {feature}\")\n\n# Adjust layout for better visualization\nplt.tight_layout()\nplt.show()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-27T20:26:31.106197Z","iopub.execute_input":"2024-11-27T20:26:31.106637Z","iopub.status.idle":"2024-11-27T20:26:32.296236Z","shell.execute_reply.started":"2024-11-27T20:26:31.106589Z","shell.execute_reply":"2024-11-27T20:26:32.295092Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Most of the features have %50 of their data distributed in a fairly compacted area and they have many outliers extending in range of the data. This suggests all features have a Gaussian distribution so I take back what I said about removing them in this case. These outliers may reflect natural variation in the data. Given that I don't have any knowledge about the data, I will be keeping these outliers. If the model accuracy decreases I can experiment with removing them to improve the model.","metadata":{}},{"cell_type":"markdown","source":"# Building the Model","metadata":{}},{"cell_type":"markdown","source":"I will use RandomForestClassifier for the first model as tree-based models are a solid choice for data including outliers.","metadata":{}},{"cell_type":"code","source":"from sklearn.ensemble import RandomForestClassifier\nfrom sklearn.model_selection import RandomizedSearchCV\n\nmodel_1 = RandomForestClassifier(random_state=0)# Initiate the model\n\nX_train = final_data\ny_train = data_train[\"target\"]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-27T20:26:32.299780Z","iopub.execute_input":"2024-11-27T20:26:32.300702Z","iopub.status.idle":"2024-11-27T20:26:32.306876Z","shell.execute_reply.started":"2024-11-27T20:26:32.300646Z","shell.execute_reply":"2024-11-27T20:26:32.305565Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"I will use the RandomSearchCV method to find the best parameter in the given possibilities. I don't use grid search because the dataset is quite large and trying every possibility would not be optimal. Randomized search does a similar thing with the grid search but doesn't try every combination, rather it tries random combinations.","metadata":{}},{"cell_type":"code","source":"\n# Define the parameter grid for RandomizedSearchCV\nparam_distributions = {\n    'n_estimators': [50, 100, 200, 300],  # Number of trees in the forest\n    'max_depth': [None, 10, 20, 30, 50],  # Maximum depth of the tree\n    'min_samples_split': [2, 5, 10],      # Minimum samples to split an internal node\n    'min_samples_leaf': [1, 2, 4],        # Minimum samples at a leaf node\n    'bootstrap': [True, False]            # Whether bootstrap samples are used\n}\n\n# Set up RandomizedSearchCV\nrandom_search = RandomizedSearchCV(\n    estimator=model_1,\n    param_distributions=param_distributions,\n    n_iter=10,           # Number of random combinations to try\n    cv=5,                # 5-fold cross-validation\n    scoring='accuracy',  # Use accuracy as the scoring metric\n    n_jobs=-1,           # Use all processors\n    random_state=0,      \n)\n\n# Fit the model to find the best parameters\nrandom_search.fit(X_train, y_train)\n\n# Drop the 'id' column from the test dataset\ndata_test_ids = data_test['id']  # Store the 'id' column separately before dropping it\ndata_test = data_test.drop(columns=[\"id\"])  # Drop the 'id' column first\n\n# Standardize the features in the test dataset\nscaled_features_test = scaler.transform(data_test)  # Using the same scaler fitted on training data\nscaled_data_test = pd.DataFrame(scaled_features_test, columns=[\"feature_05\",\"feature_20\",\"feature_08\",\"feature_01\"])\n\n# Drop the same columns as done for the training dataset\nfinal_test_data = scaled_data_test.drop(columns=high_vif_values)\n\n\nbest_rf = random_search.best_estimator_\ny_pred_1 = best_rf.predict(final_test_data)\n\n\n# Create the submission DataFrame\nsubmission = pd.DataFrame({\n    'id': data_test_ids,  # Use the stored 'id' values\n    'yield': y_pred_1\n})\n\n# Save the submission file\nsubmission.to_csv('submission1.csv', index=False)\n\nprint(\"Submission file created: submission1.csv\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-27T20:26:32.308403Z","iopub.execute_input":"2024-11-27T20:26:32.308755Z","iopub.status.idle":"2024-11-27T20:30:06.520391Z","shell.execute_reply.started":"2024-11-27T20:26:32.308721Z","shell.execute_reply":"2024-11-27T20:30:06.518659Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"I would use the something like XGBoost classifier for the second model but I won't do that now. Because all the remaining features have a Gaussian distribution and I won't miss the opportunity to use the Gaussian Naive Bayes (GNB) classifier. GNB classifier assumes that the distribution of all features is Gaussian and all features are conditionally independent. The second assumption doesn't fit our case completely but given that I removed some of the correlated features and the features weren't that much correlated at the beginning, I think that assumption wouldn't hurt.","metadata":{}},{"cell_type":"code","source":"from sklearn.naive_bayes import GaussianNB\n\n# Initialize Gaussian Naive Bayes\ngnb = GaussianNB()\n\n# Train the model\ngnb.fit(X_train, y_train)\n\n# Make predictions\ny_pred_2 = gnb.predict(final_test_data)\n\n\n# Create the submission DataFrame\nsubmission = pd.DataFrame({\n    'id': data_test_ids,\n    'target': y_pred_2\n})\n\n# Save the submission file\nsubmission.to_csv('submission2.csv', index=False)\n\nprint(\"Submission file created: submission2.csv\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-27T20:30:06.521801Z","iopub.status.idle":"2024-11-27T20:30:06.522284Z","shell.execute_reply.started":"2024-11-27T20:30:06.522048Z","shell.execute_reply":"2024-11-27T20:30:06.522091Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Since GNB classifier is a simple model, it doesn't require custom parameters as other models do. So just initializing and using it is sufficient.","metadata":{}},{"cell_type":"markdown","source":"# Conclusion\n\nThe accuracy score for the RandomForestClassifier is 86%, while for the Gaussian Naive Bayes (GNB), it is 66%. This likely indicates that the independence assumption of the GNB does not align well with our data. However, GNB is significantly more computationally efficient than the RandomForestClassifier. Therefore, in scenarios where computational efficiency is a priority and high accuracy is not critical, GNB might still be a feasible choice.","metadata":{}}]}