{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":89393,"databundleVersionId":10297209,"sourceType":"competition"}],"dockerImageVersionId":30804,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Setting Up Work Environment","metadata":{}},{"cell_type":"code","source":"import numpy as np # linear algebra\nimport pandas as pd # data processing\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2024-12-11T17:05:03.238159Z","iopub.execute_input":"2024-12-11T17:05:03.239341Z","iopub.status.idle":"2024-12-11T17:05:06.497224Z","shell.execute_reply.started":"2024-12-11T17:05:03.239248Z","shell.execute_reply":"2024-12-11T17:05:06.495791Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"data_train = pd.read_csv(\"/kaggle/input/math482-2024-2025-1-hw-03/train.csv\") #reading train data\ndata_test = pd.read_csv(\"/kaggle/input/math482-2024-2025-1-hw-03/test.csv\") #reading test data","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-11T17:05:06.500043Z","iopub.execute_input":"2024-12-11T17:05:06.500758Z","iopub.status.idle":"2024-12-11T17:05:06.717251Z","shell.execute_reply.started":"2024-12-11T17:05:06.500706Z","shell.execute_reply":"2024-12-11T17:05:06.715986Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Overview of the Data","metadata":{}},{"cell_type":"markdown","source":"**Training Data**\n\nThe training dataset contains 30,000 rows and 21 features.\n\n**Features**:  Include abstract column names that suggest no information about the data. 8 of the 21 features have categorical values and the rest has numerical values.\n\n**Target Variable**:  Numerical values.\n\n**Missing Values**:  All features have missing values. Number of the missing rows is <3000 for each row. ","metadata":{}},{"cell_type":"code","source":"data_train.info()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-11T17:05:06.719170Z","iopub.execute_input":"2024-12-11T17:05:06.719662Z","iopub.status.idle":"2024-12-11T17:05:06.768234Z","shell.execute_reply.started":"2024-12-11T17:05:06.719613Z","shell.execute_reply":"2024-12-11T17:05:06.766837Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Check for missing values in the dataset\nmissing_values = data_train.isnull().sum()\n\n# Display only columns with missing values\nmissing_columns = missing_values[missing_values > 0]\n\nif missing_columns.empty:\n    print(\"No missing values in the dataset.\")\nelse:\n    print(\"Missing values found:\")\n    print(missing_columns)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-11T17:05:06.771732Z","iopub.execute_input":"2024-12-11T17:05:06.772152Z","iopub.status.idle":"2024-12-11T17:05:06.795770Z","shell.execute_reply.started":"2024-12-11T17:05:06.772117Z","shell.execute_reply":"2024-12-11T17:05:06.794345Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"**Test Data**\n\nThe test dataset contains 10,000 rows and 21 features and there are missing values in each feature. It matches the structure of the training data.","metadata":{}},{"cell_type":"code","source":"data_test.info()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-11T17:05:06.797495Z","iopub.execute_input":"2024-12-11T17:05:06.798012Z","iopub.status.idle":"2024-12-11T17:05:06.817809Z","shell.execute_reply.started":"2024-12-11T17:05:06.797963Z","shell.execute_reply":"2024-12-11T17:05:06.816496Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Check for missing values in the dataset\nmissing_values = data_test.isnull().sum()\n\n# Display only columns with missing values\nmissing_columns = missing_values[missing_values > 0]\n\nif missing_columns.empty:\n    print(\"No missing values in the dataset.\")\nelse:\n    print(\"Missing values found:\")\n    print(missing_columns)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-11T17:05:06.819228Z","iopub.execute_input":"2024-12-11T17:05:06.819649Z","iopub.status.idle":"2024-12-11T17:05:06.837075Z","shell.execute_reply.started":"2024-12-11T17:05:06.819615Z","shell.execute_reply":"2024-12-11T17:05:06.835424Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"**Distribution of the Features**\n\nI will use histograms to see the distribution of the features ","metadata":{}},{"cell_type":"code","source":"# Drop 'id' column as it is not a feature\n#data_train = data_train.drop(columns=['id'])\n\n# Separate numeric and categorical features\nnumeric_features = data_train.select_dtypes(include=['number']).columns.tolist()\ncategorical_features = data_train.select_dtypes(include=['object']).columns.tolist()\nnumeric_features.remove(\"target\")\n\n\n# Plot distributions for numeric features\n# Drop 'id' column as it is not a feature\ndata_train = data_train.drop(columns=['id'])\n\n# Separate numeric and categorical features\nnumeric_features = data_train.select_dtypes(include=['number']).columns.tolist()\ncategorical_features = data_train.select_dtypes(include=['object']).columns.tolist()\ntarget = data_train[\"target\"]\nnumeric_features.remove(\"target\")\n\n# Set the grid size (rows x cols)\nn_cols = 2  # Number of columns in the grid\nn_rows = -(-len(numeric_features) // n_cols)  # Calculate required rows (ceiling division)\n\n# Create subplots for numeric features\nfig, axes = plt.subplots(n_rows, n_cols, figsize=(15, 5 * n_rows))  # Adjust figsize for better visibility\n\n# Loop through each numeric feature to plot\nfor i, feature in enumerate(numeric_features):\n    row, col = divmod(i, n_cols)  # Calculate row, col position in the grid\n    ax = axes[row, col] if n_rows > 1 else axes[col]  # Handle 1-row case\n    \n    sns.histplot(data_train[feature], kde=True, bins=30, color='blue', edgecolor='black', alpha=0.7, ax=ax)\n    ax.set_title(f'Distribution of {feature}')\n    ax.set_xlabel(feature)\n    ax.set_ylabel('Frequency')\n\n# Remove any empty subplots (if features < total grid slots)\nfor i in range(len(numeric_features), n_rows * n_cols):\n    fig.delaxes(axes.flatten()[i])\n\nplt.tight_layout()\nplt.show()\n\n# Plot distributions for categorical features\n\n# Set the grid size (rows x cols)\nn_cols = 2  # Number of columns in the grid\nn_rows = -(-len(categorical_features) // n_cols)  # Calculate required rows (ceiling division)\n\n# Create subplots for categorical features\nfig, axes = plt.subplots(n_rows, n_cols, figsize=(15, 5 * n_rows))  # Adjust figsize for better visibility\n\n# Loop through each categorical feature to plot\nfor i, feature in enumerate(categorical_features):\n    row, col = divmod(i, n_cols)  # Calculate row, col position in the grid\n    ax = axes[row, col] if n_rows > 1 else axes[col]  # Handle 1-row case\n    \n    sns.countplot(x=data_train[feature], palette='Set2', ax=ax)\n    ax.set_title(f'Distribution of {feature}')\n    ax.set_xlabel(feature)\n    ax.set_ylabel('Count')\n    ax.tick_params(axis='x', rotation=45)  # Rotate x-axis labels for readability\n\n# Remove any empty subplots (if features < total grid slots)\nfor i in range(len(categorical_features), n_rows * n_cols):\n    fig.delaxes(axes.flatten()[i])\n\nplt.tight_layout()\nplt.show()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-11T17:05:06.838853Z","iopub.execute_input":"2024-12-11T17:05:06.839747Z","iopub.status.idle":"2024-12-11T17:05:15.175330Z","shell.execute_reply.started":"2024-12-11T17:05:06.839692Z","shell.execute_reply":"2024-12-11T17:05:15.173991Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"We can see that numerical features are extremely skewed. These are not going to work well with the regression algorithms because regression algorithms assume the data is normally distributed. We could remove the outliers and consider the data where the majority of the values are gathered but doing so would remove most of the data because the rows where the outliers are may be different for each feature. We can also see that the feature_17 is categorical even though it has numerical values.","metadata":{}},{"cell_type":"markdown","source":"I will use the Yeo-Jhonson transformation method to reduce the skewness of the numerical features (except feature_17) and check their distributions again to see if it has worked. I use this method because it finds the best power transformation of the data and also handles negative and zero values that are present in the data.","metadata":{}},{"cell_type":"code","source":"from sklearn.preprocessing import PowerTransformer\n\n\ncategorical_features.append(\"feature_17\")\nnumeric_features.remove(\"feature_17\")\n\n\n# Initialize PowerTransformer for Yeo-Johnson\npt = PowerTransformer(method='yeo-johnson', standardize=False)  # standardize=False keeps the original scale\n\n# Apply the transformation\ndata_train[numeric_features] = pt.fit_transform(data_train[numeric_features])\n\n# Print the lambda values (one per feature)\nprint(\"Optimal lambda values for each feature:\")\nfor feature, lambda_value in zip(numeric_features, pt.lambdas_):\n    print(f\"{feature}: {lambda_value}\")\n\n# Check skewness before and after transformation\nfrom scipy.stats import skew\nprint(\"\\nSkewness after Yeo-Johnson transformation:\")\nfor feature in numeric_features:\n    print(f\"{feature}: {skew(data_train[feature].dropna())}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-11T17:05:15.176826Z","iopub.execute_input":"2024-12-11T17:05:15.177187Z","iopub.status.idle":"2024-12-11T17:05:15.855928Z","shell.execute_reply.started":"2024-12-11T17:05:15.177153Z","shell.execute_reply":"2024-12-11T17:05:15.854320Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Let's plot the numerical features again to see if the transformation helped.","metadata":{}},{"cell_type":"code","source":"# Set the grid size (rows x cols)\nn_cols = 2  # Number of columns in the grid\nn_rows = -(-len(numeric_features) // n_cols)  # Calculate required rows (ceiling division)\n\n# Create subplots\nfig, axes = plt.subplots(n_rows, n_cols, figsize=(15, 5 * n_rows))  # Adjust figsize for better visibility\n\n# Loop through each feature to plot\nfor i, feature in enumerate(numeric_features):\n    row, col = divmod(i, n_cols)  # Calculate row, col position in the grid\n    ax = axes[row, col] if n_rows > 1 else axes[col]  # Handle 1-row case\n    \n    sns.histplot(data_train[feature], kde=True, bins=30, color='blue', edgecolor='black', alpha=0.7, ax=ax)\n    ax.set_title(f'Distribution of {feature}')\n    ax.set_xlabel(feature)\n    ax.set_ylabel('Frequency')\n\n# Remove any empty subplots (if features < total grid slots)\nfor i in range(len(numeric_features), n_rows * n_cols):\n    fig.delaxes(axes.flatten()[i])\n\nplt.tight_layout()\nplt.show()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-11T17:05:15.857420Z","iopub.execute_input":"2024-12-11T17:05:15.857750Z","iopub.status.idle":"2024-12-11T17:05:22.153643Z","shell.execute_reply.started":"2024-12-11T17:05:15.857721Z","shell.execute_reply":"2024-12-11T17:05:22.152179Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"**Plotting features**\n\nI will plot all of the features in the training data with the target column to see the relation between them. I will use scatter plots with aggregated line plots for the numerical values and violin plots for the categorical values.","metadata":{}},{"cell_type":"code","source":"\n\n# Set the grid size (rows x cols)\nn_cols = 2  # Number of columns in the grid\nn_rows = -(-len(numeric_features) // n_cols)  # Calculate required rows (ceiling division)\n\nfig, axes = plt.subplots(n_rows, n_cols, figsize=(15, 5 * n_rows))  # Adjust figsize for better visibility\n\nfor i, feature in enumerate(numeric_features):\n    row, col = divmod(i, n_cols)  # Calculate row, col position in the grid\n    ax = axes[row, col] if n_rows > 1 else axes[col]  # Handle 1-row case\n    \n    # Sort by the feature\n    sorted_data = data_train[[feature, \"target\"]].sort_values(by=feature)\n    \n    # Use a rolling window to compute the mean\n    window_size = 100  # Number of points for rolling average\n    sorted_data['rolling_mean'] = sorted_data[\"target\"].rolling(window=window_size, min_periods=1).mean()\n    \n    ax.plot(sorted_data[feature], sorted_data['rolling_mean'], color='orange', label='Smoothed Target')\n    ax.scatter(sorted_data[feature], sorted_data[\"target\"], alpha=0.2, label='Raw Data', color='blue')\n    ax.set_xlabel(feature)\n    ax.set_ylabel(\"target\")\n    ax.set_title(f'Smoothed Line of {\"target\"} vs {feature} (Window={window_size})')\n    ax.legend()\n\n# Remove any empty subplots (if features < total grid slots)\nfor i in range(len(numeric_features), n_rows * n_cols):\n    fig.delaxes(axes.flatten()[i])\n\nplt.tight_layout()\nplt.show()\n\n# Plot violin plots for categorical features\n# Set the grid size (rows x cols)\nn_cols = 2  # Number of columns in the grid\nn_rows = -(-len(categorical_features) // n_cols)  # Calculate required rows (ceiling division)\n\nfig, axes = plt.subplots(n_rows, n_cols, figsize=(15, 5 * n_rows))  # Adjust figsize for better visibility\n\nfor i, feature in enumerate(categorical_features):\n    row, col = divmod(i, n_cols)  # Calculate row, col position in the grid\n    ax = axes[row, col] if n_rows > 1 else axes[col]  # Handle 1-row case\n    \n    sns.violinplot(x=data_train[feature], y=data_train[\"target\"], data=data_train, ax=ax)\n    ax.set_xlabel(feature)\n    ax.set_ylabel(\"target\")\n    ax.set_title(f'Violin Plot of {\"target\"} by {feature}')\n    ax.tick_params(axis='x', rotation=45)  # Rotate x-axis labels for better visibility\n\n# Remove any empty subplots (if features < total grid slots)\nfor i in range(len(categorical_features), n_rows * n_cols):\n    fig.delaxes(axes.flatten()[i])\n\nplt.tight_layout()\nplt.show()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-11T17:05:22.158380Z","iopub.execute_input":"2024-12-11T17:05:22.159430Z","iopub.status.idle":"2024-12-11T17:05:34.386170Z","shell.execute_reply.started":"2024-12-11T17:05:22.159375Z","shell.execute_reply":"2024-12-11T17:05:34.384830Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Data Preprocessing","metadata":{}},{"cell_type":"markdown","source":"**Handling missing values**\n\nI will handle the missing values first because most of the preprocessing methods doesn't handle the missing values. I will use SimpleImputer for imputing both categorical and numeric data. But I will use \"mean\" as strategy for numeric data and \"most_frequent\" for categorical data.","metadata":{}},{"cell_type":"code","source":"from sklearn.impute import SimpleImputer\n\n# Separate the data into numeric and categorical parts\nX_numeric = data_train[numeric_features]\nX_categorical = data_train[categorical_features]\n\n# **Impute numeric features** (mean or median)\nnum_imputer = SimpleImputer(strategy='mean')  # Change 'mean' to 'median' if needed\nX_numeric_imputed = pd.DataFrame(num_imputer.fit_transform(X_numeric), columns=numeric_features)\n\n# **Impute categorical features** (most frequent or constant)\ncat_imputer = SimpleImputer(strategy='most_frequent')  # Use strategy='constant' for 'missing' or 'unknown'\nX_categorical_imputed = pd.DataFrame(cat_imputer.fit_transform(X_categorical), columns=categorical_features)\n\n# **Combine numeric and categorical parts**\ndata_train_imputed = pd.concat([X_numeric_imputed, X_categorical_imputed, data_train[['target']]], axis=1)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-11T17:05:34.388139Z","iopub.execute_input":"2024-12-11T17:05:34.388611Z","iopub.status.idle":"2024-12-11T17:05:34.771023Z","shell.execute_reply.started":"2024-12-11T17:05:34.388571Z","shell.execute_reply":"2024-12-11T17:05:34.769692Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"**Handling Non-Numeric Values**\n\nWe should turn every non-numeric data into numeric because most models and feature selection methods don't work with non-numeric data. Since we don't have any information about the features, I will use One-Hot-Encoding just be safe.","metadata":{}},{"cell_type":"code","source":"from sklearn.preprocessing import OneHotEncoder\n\nencoder = OneHotEncoder(sparse= False )  # drop='first' to avoid multicollinearity\nX_categorical_encoded = pd.DataFrame(encoder.fit_transform(data_train_imputed[categorical_features]))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-11T17:05:34.772729Z","iopub.execute_input":"2024-12-11T17:05:34.773207Z","iopub.status.idle":"2024-12-11T17:05:34.880060Z","shell.execute_reply.started":"2024-12-11T17:05:34.773161Z","shell.execute_reply":"2024-12-11T17:05:34.878291Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"**Feature Selection**\n\nAs we can see from the relation graphs above, some of the features are irrelevant to the target value. We should remove them to reduce the noise generated. I will use Value Inflation Variance(VIF) and correlation coefficient methods to find irrelevant numeric features and use mutual info regression method to find irrelevant categorical features.","metadata":{}},{"cell_type":"code","source":"from sklearn.feature_selection import mutual_info_regression\n\n# Calculate the mutual information score for each categorical feature\nmi_scores = mutual_info_regression(X_categorical_encoded, data_train['target'])\n\n# Print the scores for each feature\nfor i, feature in enumerate(categorical_features):\n    print(f'Mutual Information Score for {feature}: {mi_scores[i]}')\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-11T17:05:34.881586Z","iopub.execute_input":"2024-12-11T17:05:34.881922Z","iopub.status.idle":"2024-12-11T17:05:57.507682Z","shell.execute_reply.started":"2024-12-11T17:05:34.881892Z","shell.execute_reply":"2024-12-11T17:05:57.506216Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"We can conclude that all of the categorical data are irrelevant to the target. So I will exclude them.","metadata":{}},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n\nX_with_target = X_numeric_imputed.copy()\nX_with_target['target'] = target  #\n\n# 1. Calculate the correlation matrix (including the target)**\ncorrelation_matrix = X_with_target.corr()\n\n# 2. Plot a heatmap for the correlation matrix (including the target)**\nplt.figure(figsize=(12, 8))\nsns.heatmap(correlation_matrix, annot=False, cmap=\"coolwarm\")\nplt.title(\"Correlation Matrix (Including Target)\")\nplt.show()\n\n# 3. Flatten the correlation matrix, keeping only the upper triangle (to avoid duplicates)**\ncorrelation_pairs = correlation_matrix.where(np.triu(np.ones(correlation_matrix.shape), k=1).astype(bool))\n\n# 4. Unstack the upper triangle into a DataFrame**\ncorrelated_features = correlation_pairs.unstack().dropna().abs().sort_values(ascending=False)\n\n# 5. Filter pairs with high correlation (include target correlations)**\nhigh_correlation_pairs = correlated_features[correlated_features > 0.1]\n\n# 6. Display the most correlated feature pairs (include the target)**\nprint(\"\\n--- Most Correlated Features ---\\n\")\nprint(high_correlation_pairs)\n\n# 7. Show the correlation of each feature with the target**\ntarget_correlations = correlation_matrix['target'].drop(labels=['target'], errors='ignore')  # Drop 'target' to avoid self-correlation\ntarget_correlations = target_correlations.abs().sort_values(ascending=False)\n\nprint(\"\\n--- Correlation of Features with Target ---\\n\")\nprint(target_correlations)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-11T17:05:57.509797Z","iopub.execute_input":"2024-12-11T17:05:57.510376Z","iopub.status.idle":"2024-12-11T17:05:58.017875Z","shell.execute_reply.started":"2024-12-11T17:05:57.510323Z","shell.execute_reply":"2024-12-11T17:05:58.016665Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nfrom statsmodels.stats.outliers_influence import variance_inflation_factor\n\n\n\n\n# **2. Calculate VIF for each feature**\ndef calculate_vif(dataframe):\n    \"\"\"Calculate Variance Inflation Factor (VIF) for each feature in the DataFrame.\"\"\"\n    vif_data = pd.DataFrame()\n    vif_data[\"Feature\"] = dataframe.columns\n    vif_data[\"VIF\"] = [variance_inflation_factor(dataframe.values, i) for i in range(dataframe.shape[1])]\n    return vif_data\n\n# Get VIF for all features\nvif_results = calculate_vif(X_numeric_imputed)\n\n# **3. Filter features with VIF > 1 and display the VIF values**\n\nprint(\"\\n--- Features with their VIF Values ---\\n\")\nprint(vif_results)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-11T17:05:58.019205Z","iopub.execute_input":"2024-12-11T17:05:58.019588Z","iopub.status.idle":"2024-12-11T17:05:58.679683Z","shell.execute_reply.started":"2024-12-11T17:05:58.019556Z","shell.execute_reply":"2024-12-11T17:05:58.678297Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"We can see that feature_08, feature_01, and feature_10 have very high vif values which indicates that they are the linear combination of multiple features. However, based on the correlation matrix above, we only have 3 relevant features in which feature_08 is the most important of them. So, I will not remove the feature_08 because I will only select three features (feature_08, feature_04, feature_15), and the other two features are not correlated to the feature_08. I know that because if they were, their vif value should have also been high.","metadata":{}},{"cell_type":"code","source":"relevant_features = X_numeric_imputed[[\"feature_04\",\"feature_08\",\"feature_15\"]].copy()\n\n\nrelevant_features.info()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-11T17:05:58.681420Z","iopub.execute_input":"2024-12-11T17:05:58.682032Z","iopub.status.idle":"2024-12-11T17:05:58.710148Z","shell.execute_reply.started":"2024-12-11T17:05:58.681967Z","shell.execute_reply":"2024-12-11T17:05:58.707722Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"I will preprocess the test data as well using the methods above","metadata":{}},{"cell_type":"code","source":"test_data_numeric = data_test[numeric_features]\n\ntest_data_imputed = pd.DataFrame(num_imputer.fit_transform(test_data_numeric), columns=numeric_features)\n\ntest_data_final = test_data_imputed[[\"feature_04\",\"feature_08\",\"feature_15\"]]\n\ntest_data_final.info()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-11T17:05:58.711978Z","iopub.execute_input":"2024-12-11T17:05:58.715783Z","iopub.status.idle":"2024-12-11T17:05:58.772013Z","shell.execute_reply.started":"2024-12-11T17:05:58.715695Z","shell.execute_reply":"2024-12-11T17:05:58.770616Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"**Not Handling of the Outliers**\n\nI will not touch the outliers because I don't have any information about the data and they can be showing natural variance.","metadata":{}},{"cell_type":"markdown","source":"# Building the Model","metadata":{}},{"cell_type":"markdown","source":"**Linear Regression**\n\nI already did the feature selection so I will not be optimizing parameters for linear regression. Because the parameters of linear regression are usually about optimizing the feature selection. However, I will be scaling the data since it is important for regression algorithms. Because the features with higher values would dominate the prediction of the linear regression.","metadata":{}},{"cell_type":"code","source":"from sklearn.linear_model import LinearRegression\nfrom sklearn.preprocessing import StandardScaler\n\n\nscaler = StandardScaler()\n\nX_train_scaled = scaler.fit_transform(relevant_features)\nX_test_scaled = scaler.transform(test_data_final)\n\n# Initialize and train the Linear Regression model\nlinear_model = LinearRegression()\nlinear_model.fit(X_train_scaled, target)\n\ny_pred1 = linear_model.predict(X_test_scaled)\n\n# Create the submission DataFrame\nsubmission1 = pd.DataFrame({\n    'id': data_test[\"id\"],\n    'target': y_pred1\n})\n\n# Save the submission file\nsubmission1.to_csv('submission_linear.csv', index=False)\n\nprint(\"Submission file created: submission_linear.csv\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-11T17:05:58.777425Z","iopub.execute_input":"2024-12-11T17:05:58.778149Z","iopub.status.idle":"2024-12-11T17:05:58.889925Z","shell.execute_reply.started":"2024-12-11T17:05:58.778082Z","shell.execute_reply":"2024-12-11T17:05:58.888325Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"**Polynomial Regression**","metadata":{}},{"cell_type":"markdown","source":"Again, I will scale the data before linear regression so that large values don't dominate the prediction.","metadata":{}},{"cell_type":"code","source":"from sklearn.preprocessing import PolynomialFeatures\n\npoly = PolynomialFeatures(degree=2, include_bias=False)\nX_poly_train = poly.fit_transform(relevant_features)\nX_poly_test = poly.transform(test_data_final)\n\n\nscaler_poly = StandardScaler()\n\nX_train_scaled_poly = scaler_poly.fit_transform(X_poly_train)\nX_test_scaled_poly = scaler_poly.transform(X_poly_test)\n\npoly_model = LinearRegression()\npoly_model.fit(X_train_scaled_poly, target)\n\n\n# Make predictions\ny_pred2 = poly_model.predict(X_test_scaled_poly)\n\n\n# Create the submission DataFrame\nsubmission2 = pd.DataFrame({\n    'id': data_test[\"id\"],\n    'target': y_pred2\n})\n\n# Save the submission file\nsubmission2.to_csv('submission_polynomial.csv', index=False)\n\nprint(\"Submission file created: submission_polynomial.csv\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-11T17:05:58.891277Z","iopub.execute_input":"2024-12-11T17:05:58.891768Z","iopub.status.idle":"2024-12-11T17:05:59.025283Z","shell.execute_reply.started":"2024-12-11T17:05:58.891685Z","shell.execute_reply":"2024-12-11T17:05:59.021165Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"**Support Vector Regression**\n\nSupport Vector Regression (SVR) relies on concepts like distances, margins, and kernels, which are directly affected by the scale of both the features and the target. So we have to scale both the features and target values unlike in linear regression. After making the prediction I will reverse the scaling for the predicted target so that the values are accurate. I will also use grid search method to find the best parameters for the SVR.","metadata":{}},{"cell_type":"code","source":"from sklearn.model_selection import RandomizedSearchCV\nfrom sklearn.svm import SVR\n\n\n# Scale features (X) and target (y)\nscaler_X = StandardScaler()\nscaler_y = StandardScaler()\n\n# Scale the features (X)\nX_train_scaled = scaler_X.fit_transform(relevant_features)\nX_test_scaled = scaler_X.transform(test_data_final)\n\n# Scale the target (y) (reshape needed for 1D arrays)\ny_train_scaled = scaler_y.fit_transform(target.values.reshape(-1, 1)).ravel()  # y must be reshaped for scaling\n\n# Define the parameter distribution\nparam_dist = {\n    'C': [1, 10, 100],        # Regularization\n    'epsilon': [0.1, 0.5, 1], # Margin of tolerance\n    'kernel': ['rbf'],          # Kernel type\n    'gamma': ['scale', 'auto'] # Kernel coefficient\n}\n\n# Initialize the SVR model\nsvr = SVR()\n\n# Perform randomized search\nrandom_search = RandomizedSearchCV(estimator=svr, param_distributions=param_dist, n_iter=5, scoring='neg_mean_squared_error', cv=3, verbose=2)\nrandom_search.fit(X_train_scaled, y_train_scaled)\n\n# Get the best parameters and best model\nbest_params = random_search.best_params_\nbest_model = random_search.best_estimator_\n\nprint(\"Best parameters found:\", best_params)\n\n# Make predictions using the best model\ny_pred_scaled = best_model.predict(X_test_scaled)\n\n# Reverse scaling for the target\ny_pred3 = scaler_y.inverse_transform(y_pred_scaled.reshape(-1, 1)).ravel()\n\n\n# Create the submission file\nsubmission3 = pd.DataFrame({\n    'id': data_test[\"id\"],\n    'target': y_pred3\n})\nsubmission3.to_csv('submission_vector_optimized.csv', index=False)\n\nprint(\"Submission file created: submission_vector_optimized.csv\")\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-11T17:05:59.031134Z","iopub.execute_input":"2024-12-11T17:05:59.032377Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"**Neural Network**\n\nI will scale the features and the target as similar to the above so that NN converges faster. I will also use the Keras tuner to automatically find the best parameters for the NN.","metadata":{}},{"cell_type":"code","source":"import tensorflow as tf\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Dense\nimport keras_tuner as kt  # Import Keras Tuner\n\n\n# Scale the features and target\nscaler_X = StandardScaler()\nscaler_y = StandardScaler()\n\nX_train_scaled = scaler_X.fit_transform(relevant_features)\nX_test_scaled = scaler_X.transform(test_data_final)\n\ny_train_scaled = scaler_y.fit_transform(target.values.reshape(-1, 1)).ravel()\n\n# Define the model-building function for Keras Tuner\ndef build_model(hp):\n    model = Sequential()\n    \n    # First hidden layer: Tune the number of neurons\n    model.add(Dense(\n        units=hp.Int('units_layer_1', min_value=8, max_value=64, step=8),\n        activation='relu',\n        input_dim=3  # Input dimension (number of features)\n    ))\n    \n    # Second hidden layer: Tune the number of neurons\n    model.add(Dense(\n        units=hp.Int('units_layer_2', min_value=8, max_value=64, step=8),\n        activation='relu'\n    ))\n    \n    # Output layer\n    model.add(Dense(1))  # Single output for regression\n\n    # Compile the model: Tune the learning rate\n    model.compile(\n        optimizer=tf.keras.optimizers.Adam(\n            hp.Choice('learning_rate', values=[1e-2, 1e-3, 1e-4])\n        ),\n        loss='mse',\n        metrics=['mae']\n    )\n    return model\n\n# Initialize the Keras Tuner\ntuner = kt.Hyperband(\n    build_model,\n    objective='val_loss',  # Optimize for validation loss\n    max_epochs=50,  # Maximum number of epochs for training\n    factor=3,  # Factor to reduce the number of configurations\n    directory='my_tuner_dir',  # Directory to save tuning results\n    project_name='NN_tuning'  # Name of the tuning project\n)\n\n# Perform hyperparameter search\ntuner.search(X_train_scaled, y_train_scaled, epochs=50, validation_split=0.2, batch_size=32, verbose=1)\n\n# Get the best hyperparameters\nbest_hps = tuner.get_best_hyperparameters(num_trials=1)[0]\n\nprint(f\"Best number of neurons in layer 1: {best_hps.get('units_layer_1')}\")\nprint(f\"Best number of neurons in layer 2: {best_hps.get('units_layer_2')}\")\nprint(f\"Best learning rate: {best_hps.get('learning_rate')}\")\n\n# Train the model with the best hyperparameters\nbest_model = tuner.hypermodel.build(best_hps)\nhistory = best_model.fit(X_train_scaled, y_train_scaled, epochs=100, validation_split=0.2, batch_size=32, verbose=1)\n\n# Evaluate the model on the test set\ny_pred_scaled = best_model.predict(X_test_scaled)\ny_pred4 = scaler_y.inverse_transform(y_pred_scaled).ravel()\n\n\n# 8. Create the submission file\nsubmission = pd.DataFrame({\n    'id': data_test[\"id\"],\n    'target': y_pred4.ravel()\n})\nsubmission.to_csv('submission_NN_tuned.csv', index=False)\n\nprint(\"Submission file created: submission_NN_tuned.csv\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Conclusion\n\nWe have a very high MSE score. That is probably because of the fact that I eliminated the majority of the data in the feature selection. But that was necessary because the eliminated data wasn't correlated with the target value at all. So we can say that the data provided in this task is not optimal for predicting the target","metadata":{}}]}